{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ca6e275",
   "metadata": {},
   "source": [
    "# Notebook 2:<br> **Model Training, Tuning, and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a81e02",
   "metadata": {},
   "source": [
    "This notebook trains and optimizes classification models for the OULAD early-warning task using the preprocessed artifacts from `01_dataset_preprocessing.ipynb`.\n",
    "\n",
    "**Prediction time:** <br>\n",
    "Day `CUTOFF_DAY` (as defined in Notebook 1)  \n",
    "\n",
    "**Target:** <br>\n",
    "The classes are mapped to the `risk_tier`:\n",
    "1. Low Risk: Pass/Distinction  \n",
    "2. Medium Risk: Fail  \n",
    "3. High Risk: Withdrawn  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ed2cb5",
   "metadata": {},
   "source": [
    "## Setup Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca5251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# Standard library imports\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedGroupKFold, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
    "\n",
    "# Project utilities\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "from utils import summarize_cv, SCORING_METRICS\n",
    "\n",
    "# Constants (match Notebook 1)\n",
    "RANDOM_STATE = 42\n",
    "CUTOFF_DAY = 98     # Important: Ensure that this is the same as in Notebook 1\n",
    "\n",
    "# Directories for outputs\n",
    "# With a method to resolve outputs relative to current working directory first (robust when kernel cwd varies)\n",
    "candidate_out = Path.cwd() / \"outputs\"\n",
    "if candidate_out.exists():\n",
    "    OUT_DIR = candidate_out\n",
    "else:\n",
    "    OUT_DIR = Path(\"../outputs\")\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "TAB_DIR = OUT_DIR / \"tables\"\n",
    "DATA_DIR = OUT_DIR / \"data\"\n",
    "MODEL_DIR = OUT_DIR / \"models\"\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd530ad8",
   "metadata": {},
   "source": [
    "## Data Loading and Setup\n",
    "\n",
    "### 1. Train/Test Data\n",
    "\n",
    "Load the outputs from the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32c7c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (20284, 37)\n",
      "X_test: (5069, 37)\n",
      "y_train: (20284,)\n",
      "y_test: (5069,)\n",
      "groups_train: (20284,)\n"
     ]
    }
   ],
   "source": [
    "def load_series_csv(path: Path) -> pd.Series:\n",
    "    df = pd.read_csv(path)\n",
    "    if df.shape[1] == 1:\n",
    "        return df.iloc[:, 0]\n",
    "    return df.iloc[:, -1]\n",
    "\n",
    "# Paths:\n",
    "X_train_path = DATA_DIR / \"processed\" / \"X_train_raw.csv\"\n",
    "X_test_path  = DATA_DIR / \"processed\" / \"X_test_raw.csv\"\n",
    "y_train_path = DATA_DIR / \"processed\" / \"y_train.csv\"\n",
    "y_test_path  = DATA_DIR / \"processed\" / \"y_test.csv\"\n",
    "groups_path  = DATA_DIR / \"processed\" / \"groups_train.csv\"\n",
    "preprocess_path = MODEL_DIR / \"preprocess_pipeline.joblib\"\n",
    "\n",
    "# Load processed data:\n",
    "X_train = pd.read_csv(X_train_path)\n",
    "X_test  = pd.read_csv(X_test_path)\n",
    "y_train = load_series_csv(y_train_path)\n",
    "y_test  = load_series_csv(y_test_path)\n",
    "groups_train = load_series_csv(groups_path)\n",
    "\n",
    "preprocess = joblib.load(preprocess_path)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "print(\"groups_train:\", groups_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acf168a",
   "metadata": {},
   "source": [
    "### 2. Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b65ec279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution (%):\n",
      "risk_tier\n",
      "Low Risk       60.7\n",
      "Medium Risk    27.8\n",
      "High Risk      11.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution (%):\n",
      "risk_tier\n",
      "Low Risk       60.7\n",
      "Medium Risk    27.8\n",
      "High Risk      11.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Show the % distribution of classes in the TRAIN split\n",
    "print(\"Train class distribution (%):\")\n",
    "print((y_train.value_counts(normalize=True) * 100).round(1))  # normalize=True -> proportions; *100 -> percent\n",
    "\n",
    "# Show the % distribution of classes in the TEST split\n",
    "print(\"\\nTest class distribution (%):\")\n",
    "print((y_test.value_counts(normalize=True) * 100).round(1))\n",
    "\n",
    "# Ensure training matrices/vectors align:\n",
    "# - X_train rows must match y_train labels\n",
    "# - groups_train must have one group ID per training row (for group-aware CV)\n",
    "assert len(X_train) == len(y_train) == len(groups_train), \"Train X/y/groups lengths do not match.\"\n",
    "\n",
    "# Ensure test features and labels align (one label per test row)\n",
    "assert len(X_test) == len(y_test), \"Test X/y lengths do not match.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac040271",
   "metadata": {},
   "source": [
    "### 3. Cross-Validation Setup\n",
    "\n",
    "Cross-validation (CV) provides a more reliable estimate of model performance than a single split. The solution is using **StratifiedGroupKFold** with **Macro-F1** as a primary indicator (better for imbalanced classes). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111c0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 5-fold stratified, group-aware CV splitter\n",
    "cv = StratifiedGroupKFold(\n",
    "    n_splits=5,               # number of folds\n",
    "    shuffle=True,             # shuffle before splitting for randomness\n",
    "    random_state=RANDOM_STATE # reproducible fold assignments\n",
    ")\n",
    "\n",
    "# Scoring metrics imported from utils.py:\n",
    "# - macro_f1: averages F1 across classes\n",
    "# - balanced_acc: averages recall across classes\n",
    "# - precision_macro / recall_macro: diagnostics to understand trade-offs across classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50db56b",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### 4. Models Used\n",
    "\n",
    "1. **Logistic Regression (Multimodal)** since it models all classes jointly in a single probability framework.\n",
    "\n",
    "2. **Random Forest** since it uses an ensemble of decision trees that reduces overfitting by training many diverse trees and averaging their predictions, while still offering practical interpretability through feature-importance scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73864016",
   "metadata": {},
   "source": [
    "#### 4.1. Logistic Regression (Multimodal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2af3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(\n",
    "        max_iter=3000,\n",
    "        solver=\"saga\",\n",
    "        random_state=RANDOM_STATE\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437306d",
   "metadata": {},
   "source": [
    "#### 4.2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "66e5c885",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", RandomForestClassifier(\n",
    "        n_estimators=500,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        class_weight=\"balanced_subsample\"\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f55dcb",
   "metadata": {},
   "source": [
    "4.3. Define Models Based on Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "715f20ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"logreg_default\": logreg_pipe,\n",
    "    \"rf_default\": rf_pipe,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989104fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a816b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>macro_f1_mean</th>\n",
       "      <th>macro_f1_std</th>\n",
       "      <th>balanced_acc_mean</th>\n",
       "      <th>balanced_acc_std</th>\n",
       "      <th>precision_macro_mean</th>\n",
       "      <th>precision_macro_std</th>\n",
       "      <th>recall_macro_mean</th>\n",
       "      <th>recall_macro_std</th>\n",
       "      <th>fit_time_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rf_default</td>\n",
       "      <td>0.498384</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.512165</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.611766</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.512165</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>12.316097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logreg_default</td>\n",
       "      <td>0.455669</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.474944</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>0.562743</td>\n",
       "      <td>0.059405</td>\n",
       "      <td>0.474944</td>\n",
       "      <td>0.003341</td>\n",
       "      <td>15.534321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model  macro_f1_mean  macro_f1_std  balanced_acc_mean  balanced_acc_std  precision_macro_mean  precision_macro_std  \\\n",
       "1      rf_default       0.498384      0.002729           0.512165          0.003140              0.611766             0.021072   \n",
       "0  logreg_default       0.455669      0.003851           0.474944          0.003341              0.562743             0.059405   \n",
       "\n",
       "   recall_macro_mean  recall_macro_std  fit_time_mean  \n",
       "1           0.512165          0.003140      12.316097  \n",
       "0           0.474944          0.003341      15.534321  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = []\n",
    "for name, pipe in models.items():\n",
    "    res = cross_validate(\n",
    "        pipe,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        groups=groups_train,\n",
    "        cv=cv,\n",
    "        scoring=SCORING_METRICS,\n",
    "        return_train_score=False\n",
    "    )\n",
    "    rows.append(summarize_cv(name, res))\n",
    "\n",
    "compare_df = pd.DataFrame(rows).sort_values(\"macro_f1_mean\", ascending=False)\n",
    "compare_df.to_csv(TAB_DIR / \"table_12_model_comparison_cv_train_only.csv\", index=False)\n",
    "compare_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
